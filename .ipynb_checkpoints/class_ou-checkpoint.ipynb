{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e82dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" OU (Ornstein-Uhlenbeck) process\n",
    "\n",
    "    dX = -A(X-alpha)dt + v dB\n",
    "    \"\"\"\n",
    "class OU:\n",
    "    def __init__(self, t0, t1, z0, alpha, beta, sigma, timegrid=False, dN=500):\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        self.z0 = z0\n",
    "        self.alpha = alpha\n",
    "        \"\"\"\n",
    "            check whether every element in sigma is positive\n",
    "        \"\"\"\n",
    "        assert all(beta > 0), \"beta should be positive\"    \n",
    "        self.beta = beta\n",
    "        assert all(sigma > 0), \"variance should be positive\"\n",
    "        self.sigma = sigma\n",
    "        assert alpha.shape[0] == beta.shape[0], \"parameter dimension must be equal\"\n",
    "        self.D = alpha.shape[0]\n",
    "        if timegrid == True:\n",
    "            self.pts = torch.linspace(t0, t1, dN).repeat(D, 1)\n",
    "        else:\n",
    "            self.pts = torch.sort(torch.cat([(t1 - t0) * torch.rand(D, dN-2) + t0, torch.tensor([self.t0, self.t1]).repeat(D,1)], axis=1), axis=1)[0]\n",
    "        self.trj, self.dt = self.path()\n",
    "          \n",
    "    def path(self):\n",
    "        \"\"\" Simulates a sample path\"\"\"\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "        sigma = self.sigma\n",
    "        x0 = self.z0\n",
    "        t = self.pts\n",
    "        \n",
    "        def variance(t, beta, sigma):\n",
    "            assert all(beta >= 0)\n",
    "            assert all(sigma >= 0)\n",
    "            return sigma * sigma * (1.0 - torch.exp( -2.0 * beta * t)) / (2 * beta)\n",
    "\n",
    "        def std(t, beta, sigma):\n",
    "            return torch.sqrt(variance(t, beta, sigma))\n",
    "\n",
    "        def mean(x0, t, beta, alpha):\n",
    "            assert all(beta >= 0)\n",
    "            return x0 * torch.exp(-1 * beta * t) + (1.0 - torch.exp( -1 * beta * t)) * alpha\n",
    "\n",
    "        assert t.shape[1] > 1\n",
    "        normal = Normal(loc=0., scale=1.)\n",
    "        x = normal.sample(t.size())\n",
    "        x[:, 0] = x0\n",
    "        dt = torch.diff(t)\n",
    "        scale = std(dt, beta, sigma)\n",
    "        x[:, 1:] = x[:, 1:] * scale\n",
    "        for i in range(1, x.shape[1]):\n",
    "            x[:, i] += mean(x[:, i - 1].reshape(-1, 1), dt[:, i - 1].reshape(-1, 1), beta, alpha).flatten()\n",
    "        return x, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ff91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(t, x, params):\n",
    "    \n",
    "    \"\"\"Calculates log likelihood of a path\"\"\"\n",
    "    \n",
    "    def variance(t, beta, sigma):\n",
    "        assert all(beta >= 0)\n",
    "        assert all(sigma >= 0)\n",
    "        return sigma * sigma * (1.0 - torch.exp( -2.0 * beta * t)) / (2 * beta)\n",
    "\n",
    "    def std(t, beta, sigma):\n",
    "        return torch.sqrt(variance(t, beta, sigma))\n",
    "\n",
    "    def mean(x0, t, beta, alpha):\n",
    "        assert all(beta >= 0)\n",
    "        return x0 * torch.exp(-1 * beta * t) + (1.0 - torch.exp( -1 * beta * t)) * alpha\n",
    "\n",
    "    alpha, beta, sigma = params\n",
    "    \n",
    "\n",
    "    dt = torch.diff(t)\n",
    "    mu = mean(x[:, :-1], dt, beta, alpha)\n",
    "    var = std(dt, beta, sigma)\n",
    "    return torch.sum(Normal(loc=mu, scale=var).log_prob(x[:, 1:]), axis=1)\n",
    "\n",
    "    params_ = Variable(params, requires_grad=True)\n",
    "\n",
    "    LL = loglik(t, x, *params_)\n",
    "    LL.backward(torch.tensor([1.]).repeat(x.shape[0]))\n",
    "\n",
    "\n",
    "    return {'alpha':params_.grad[0].clone().detach(), 'beta':params_.grad[1].clone().detach()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sde = OU(0., 10., 10., *params.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ef1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sde.pts.T, sde.trj.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gradient(sde.pts, sde.trj, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, D):\n",
    "    for epoch in range(n_epochs):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        x = path(10, t.repeat(D,1), *param)\n",
    "        NLL = -1 * loglik(t, x.clone().data, a, b, c)\n",
    "        NLL.backward(torch.tensor([1.0, 1.0]))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            a -= lr * a.grad\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"alpha = \", a.data, b.data, c.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(t, x, params):\n",
    "    print(params)\n",
    "    \n",
    "    params_ = Variable(params, requires_grad=True)\n",
    "    \n",
    "    LL = loglik(t, x, *params_)\n",
    "    LL.backward(torch.tensor([1.]).repeat(x.shape[0]))\n",
    "\n",
    "    \n",
    "    return {'alpha':params_.grad[0].clone().detach(), 'beta':params_.grad[1].clone().detach()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gradient(T, obs, params.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f9378",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((D,1)) + 4.\n",
    "b = torch.randn((D,1)) + 3.\n",
    "c = torch.ones((D,1)) + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.linspace(0,10,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3cfb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Variable(torch.stack([a,b,c]), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([params], lr = 0.05)\n",
    "obs = path(10, T.repeat(D, 1), *torch.ones((3,D,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T.repeat(D,1).T, obs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, t, obs, optimizer, params):\n",
    "    for epoch in range(n_epochs):\n",
    "        NLL = -1 * loglik(t, obs, *params)\n",
    "        NLL.backward(torch.tensor([1.]).repeat(D))\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"alpha = \", *params)\n",
    "    \n",
    "    return params.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e11a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(5000, T, obs, optimizer, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70659e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "x = Variable(torch.FloatTensor([[1, 2, 3, 4]]), requires_grad=True)\n",
    "z = x ** 2\n",
    "loss = z\n",
    "\n",
    "# do backward for first element of z\n",
    "z.backward(torch.FloatTensor([[1, 0, 0, 0]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_() #remove gradient in x.grad, or it will be accumulated\n",
    "\n",
    "# do backward for second element of z\n",
    "z.backward(torch.FloatTensor([[0, 1, 0, 0]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_()\n",
    "\n",
    "# do backward for all elements of z, with weight equal to the derivative of\n",
    "# loss w.r.t z_1, z_2, z_3 and z_4\n",
    "z.backward(torch.FloatTensor([[1, 1, 1, 1]]), retain_graph=True)\n",
    "print(x.grad.data)\n",
    "x.grad.data.zero_()\n",
    "\n",
    "# or we can directly backprop using loss\n",
    "# loss.backward() # equivalent to loss.backward(torch.FloatTensor([1.0]))\n",
    "# print(x.grad.data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a83b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13613be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLL = -loglik(t,x, *params)\n",
    "NLL.backward()\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    params -= 0.0001 * params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48076b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50504586",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = torch.tensor(-10.0, requires_grad = True)\n",
    "lr = 0.005\n",
    "for i in range(3100):\n",
    "    if loc.grad is not None:\n",
    "        loc.grad.zero_()\n",
    "    to_learn = dist.Normal(loc=loc, scale=1.0)\n",
    "    loss = -torch.sum(to_learn.log_prob(uv.sample(torch.tensor([100]))))\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loc -= lr * loc.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c063cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1adce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 1.\n",
    "t0 = 0.\n",
    "dN = 30\n",
    "D = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60273b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.sort(torch.cat([(t1 - t0) * torch.rand(D, dN-2) + t0, torch.tensor([t0, t1]).repeat(D,1)], axis=1), axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbbcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diff(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eca7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "t = np.linspace(0,1,500)\n",
    "x = path(5, t, 10, 0, 1)\n",
    "plt.plot(t, x)\n",
    "\n",
    "loglik(t, x, 10,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76dab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from scipy.optimize import minimize\n",
    "from jax.scipy import optimize \n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_loglike(theta):\n",
    "    beta = theat[0]\n",
    "    sigma = jnp.exp(theta[-1])\n",
    "    mu = jnp.dot(x, beta)\n",
    "    ll = jax.numpy.sum(jax.scipy.stats.norm.logpdf(y, loc = mu, scale=sigma))\n",
    "    return (-1 * ll) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a2977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
